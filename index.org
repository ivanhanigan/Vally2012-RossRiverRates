#+TITLE:Vally2012-RossRiverRates 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* workplan 
- this is already published
- I am using this project as an example of a simple pipeline
* worklog
** 2009-12-23 results from Ivan Re: the original paper
- the versions folder has an orgmode file in it that was used to produce original paper
** 2015-11-03 results from Ivan Re: use as a simple example
- start a Rmarkdown report that shows what happens if you change a Decision
- the decision was to remove first row in urban

* workflow
** COMMENT test-newnode

#+begin_src R :session *R* :tangle no :exports none :eval yes
  projdir <- "~/projects/Vally2012-RossRiverRates"
  setwd(projdir)
  library(disentangle)
  library(stringr)
  steps <- read.csv(textConnection('
  STEP, INPUTS, OUTPUTS, DESCRIPTION
  Geocoding, private, shapefile, this was done by Mark Peel
  rates,     "shapefile, buffer", rates-ssheet, done by Mark 
  reshape long, rates-ssheet, "d_eastern, d_urban, dat2", dat2 is row binded to add urban and eastern to same table
  model1, "d_eastern, d_urban", "fit, fit1, fit1.1", Note that I decided to exclude zero pop from urban (fit1) and did this again as fit1.1 to show difference
  model2 , dat2, fit2, "This uses a multiplicative term, the variance-covariance matrix is required for B3 and SE"
  model3, dat2, fit3, This is the re-parametrisation so that coeff and se are easier
  '), stringsAsFactors = F, strip.white = T)
  ## #write.csv(steps, "workflow_steps.csv", row.names = F)
  ## steps <- read.csv("workflow_steps.csv", stringsAsFactors = F, strip.white = T)
  
  str(steps)
  nodes <- newnode(
    indat = steps,
    names_col = "STEP",
    in_col = "INPUTS",
    out_col = "OUTPUTS",
    desc_col = "DESCRIPTION",
    nchar_to_snip = 40)
  #DiagrammeR::grViz(nodes)
  
  sink("workflow_steps.dot")
  cat(nodes)
  sink()
  system("dot -Tpdf workflow_steps.dot -o workflow_steps.pdf")
  #browseURL("workflow_steps.pdf")
  
  
#+end_src

#+RESULTS:
: 0


* main.R

#+name:main
#+begin_src R :session *R* :tangle main.R :exports none :eval no :padline no
  # Project: Vally2012-RossRiverRates
  # Author: Hanigan, Ivan
  # Maintainer: <ivan.hanigan@gmail.com>
  
  # This is the main file for the project
  # It should do very little except call the other files
  
  ### Set the working directory
  projdir <- "~/projects/Vally2012-RossRiverRates"
  setwd(projdir)
  
  ### Set any global variables here
  ####################
  ylims <- c(0,10)
  
  ####################
  
  
  ### Run the code
  source("code/func.R")
  # data provided needed cleaning once only, also I am not sharing the entire original file on github
  #source("code/load1.R")
  #source("code/clean1.R")
  # load data derived 
  source("code/load.R")
  # no more cleaning done
  
  # Do some EDA, especially test the sampling from the binomial
  # distribution idea
  # NOT RUN, do this manually
  # source("code/EDA_RossRiverRates.R")
  
  # Do the RRv rates figures for paper
  source("code/do_plots_RossRiverRates.R")
  
  # Do the model checking 
  source("code/do_model_checking.R")
#+end_src

* func.R
* data provided, load and clean 
** load.R
#+name:load
#+begin_src R :session *R* :tangle code/load1.R :exports none :eval no :padline no
# Project: Vally2012-RossRiverRates
# Author: Ivan Hanigan
# Maintainer: ivan.hanigan@gmail.com

# This file loads all the libraries and data files needed
# Don't do any cleanup here

### Load in any data files
# NOT RUN, the excel sheet was read once only and used to create derived data for future work
dat <- read_excel("data_provided/RRV Erp96_MGA_EastVUrban_210208 Mark's final w MDLs additions 30 March 2008.xls", sheet='Table 1', skip =1)
str(dat)

#+end_src

** clean.R
#+name:clean
#+begin_src R :session *R* :tangle code/clean1.R :exports none :eval no :padline no
  # Project: Vally2012-RossRiverRates
  # Author: ivanhanigan
  # Maintainer: <ivan.hanigan@gmail.com>
  
  # All the potentially messy data cleanup
  dat[,1:4]
  # We limit the analysis to up to 7.5 km away
  dat <- dat[1:15,]
  str(dat)
  dat$`Buffer (km)` <- as.numeric(dat$`Buffer (km)`)
  names(dat)
   ## [1] "Buffer (km)"                      "RRV cases"
   ## [3] "Total persons"                    "Entire Leschenault RRV rate/1000"
   ## [5] "RRV cases"                        "Total persons"
   ## [7] "Eastern Estuary RRV rate/1000"    "RRV cases"
   ## [9] "Total persons"                    "Urban Bunbury RRV rate/1000"
  
  d_eastern <- dat[,c(1,5:7)]
  names(d_eastern) <- c('buffer','cases','pops','rate')
  
  d_urban <- dat[1:15,c(1,8:10)]
  names(d_urban) <- c('buffer','cases','pops','rate')
  
  # combine the urban and rural data
  d_eastern$urban <- 0
  d_urban$urban <- 1
  dat2 <- rbind(d_eastern, d_urban)
  str(dat2)
  dat2
  
  save.image()
#+end_src
* data derived, load cleaned

#+name:load
#+begin_src R :session *R* :tangle code/load.R :exports none :eval no :padline no
  # Project: Vally2012-RossRiverRates
  # Author: ivanhanigan
  # Maintainer: <ivan.hanigan@gmail.com>
  load(".RData")
  ls()
#+end_src

* do
** EDA, check sim 

#+name:EDA_RossRiverRates
#+begin_src R :session *R* :tangle code/EDA_RossRiverRates.R :exports none :eval no
# Sampling from the binomial distribution
# the cases of ross river counted in concentric buffers around a swamp

str(d_eastern)
# first model this

fit  <- glm(cases~ buffer + offset(log(pops)),family='poisson', data=d_eastern)
summary(fit)
termplot(fit,se=T,partial.resid = TRUE)
par(mfrow=c(2,2))
plot(fit)

# how compare to Hass orig logistic model?
fit_logistic <- glm(cbind(d_eastern$cases,(d_eastern$pops-d_eastern$cases)) ~ buffer,family=binomial(link = "logit"), data=d_eastern)
summary(fit_logistic)
# very close

#### URBAN ####
d_urban


# first model this
fit <- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban)
summary(fit)
# try some alternatives
require(splines)
require(mgcv)
# Exclude row with zero pop
d_urban2 <- d_urban[-1,]
fit1 <- glm(cases~ ns(buffer,df=3) + offset(log(pops)),family='poisson', data=d_urban2)
summary(fit1)
termplot(fit1, se = T)
dev.off()
fit2 <- gam(cases~ s(buffer) + offset(log(pops)),family='poisson', data=d_urban2)
summary(fit2)
plot(fit2)
dev.off()
# Looks like nothing going on in Urban

#### Try sampling from the binomial
"Description

background

The data are counts of cases of a disease in concentric buffers around a putative exposure source.

The aim is to display the measured incidence rates with 95% confidence intervals across the buffers to discern if there is a trend related to distance from exposure source.
methods

To calculate the 95% confidence intervals we will generate 1000 random numbers for each buffer zone from the binomial distribution by specifying the probability and sample size of that buffer zone.

The use of the rbinom() function follows the description in Crawley 2002 pp 476-477 (which includes a mathematical description as well).
Input

The data include cases and resident populations counted within each buffer zone using an overlay and intersect GIS operation.
Output

We want to generate a graph with distance on the x axis and incidence rate on the y axis, showing the measured rates and the estimated 95%CI.

REFERENCE:
Crawley, M.J. (2002). Statistical Computing: An Introduction to Data Analysis using S-Plus. John Wiley & Sons Ltd, Chichester.
"
# FIRST WITH FAKE DATA
# Sampling from the binomial distribution
# construct a data frame with the distances, cases and populations.
# the motivating example for this is unpublished data so for this example I'll generate random numbers from a normal distribution to simulate those data, using their (absolute) mean and standard deviation
# construct a data frame with the distances, cases and populations.
buffer=c(0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7,7.5)
cases=abs(round(rnorm(15,13,10)))
cases=cases[order(cases,decreasing=T)]
pop=abs(round(rnorm(15,2686,1122)))
pop=pop[order(pop,decreasing=T)]
d=data.frame(buffer,cases,pop)

# the rbinom function
# n = number of random numbers to be generated
# size = sample size
# prob = probability of infection

# create an empty matrix to recieve the estimated counts for each buffer
out=matrix(nrow=0,ncol=3)

# create an empty matrix to recieve the estimated CIs
ci=matrix(nrow=0,ncol=4)

# now loop through each buffer zone
for(i in 1:15){
                # step one generate 1000 random numbers for the buffer zone
                out=rbind(out,# append the new rows to the 'out' matrix
                        cbind(d[i,'buffer'], # an index for each buffer zone
                        rbinom(n=1000,size=d[i,'pop'],prob=d[i,'cases']/d[i,'pop']), # the random number generator is given the required n, sample size (pop) and probability (rate)
                        d[i,'pop'])) # when we finish we want to display the counts as a rate so need the denominator

                # step 2 calculate the two tailed lower and upper 95% expected counts given the specified probability and sample size
                ci=rbind(ci,cbind(d[i,'buffer'],
                        qbinom(p=0.025,size=d[i,'pop'],prob=d[i,'cases']/d[i,'pop']), # qbinom is the quantile function for the binomial distribution
                        qbinom(p=0.975,size=d[i,'pop'],prob=d[i,'cases']/d[i,'pop']),
                        d[i,'pop'])) # for the CIs as a rate

}

# now to generate the plot
# first plot the 1000 generated counts (as a rate/1000)
plot(out[,1],1000*(out[,2]/out[,3]),pch=16,cex=.4,ylab='rate/1000',xlab='Buffer (km)',ylim=c(0,15))
# show the area between the 95% CIs (as a rate/1000) with a grey polygon (note the need to reverse the order of the upper confidence estimates).
polygon(c(ci[,1],ci[15:0.5,1]),c(1000*(ci[,2]/ci[,4]),1000*(ci[15:.5,3]/ci[15:.5,4])),col='grey',border = NA)
# now show the 95% CI as dotted lines
lines(ci[,1],1000*(ci[,2]/ci[,4]),lty=2)
lines(ci[,1],1000*(ci[,3]/ci[,4]),lty=2)
# show the data points on top of the grey polygon
points(out[,1],1000*(out[,2]/out[,3]),pch=16,cex=.4)
# plot the empirical rate
lines(d[,1],1000*(d[,2]/d[,3]),lwd=2)

# add a legend
legend('topright',legend=c('1000 simulated rates','Empirical rate','Simulated 95%CI'),pch=c(16,NA,NA),lty=c(0,1,2),lwd=c(0,2,1))

# save out as whatever image format the journal requires
savePlot('reports/rbinom.jpg',type=c('jpeg'))

dev.off()


##########################################################################3
# NOW WITH THE DATA
# the rbinom function
# n = number of random numbers to be generated
# size = sample size
# prob = probability of infection

# create an empty matrix to recieve the estimated counts for each buffer
out=matrix(nrow=0,ncol=4)

# create an empty matrix to recieve the estimated CIs
#ci=matrix(nrow=0,ncol=4)


#################################################################################
# Do this with a manual loop, set d to the appropriate input and then repeat
par(mfrow=c(2,1))
# start second time here, change d
# and set up a title as appropriate
for(i in 1:2){
if(i == 1){
d=d_eastern
title_label <- "eastern"
} else {
d=d_urban
title_label <- "urban"
}

names(d)=c('buffer','cases','pops','rate')

# create an empty matrix to recieve the estimated counts for each buffer
out=matrix(nrow=0,ncol=4)

# now loop through each buffer zone
for(i in 1:15){
                # step one generate 1000 random numbers for the buffer zone
                out=rbind(out,# append the new rows to the 'out' matrix
                        cbind(1:1000,d[i,'buffer'], # an index for each buffer zone
                        rbinom(n=1000,size=d[i,'pops'],prob=d[i,'cases']/d[i,'pops']), # the random number generator is given the required n, sample size (pop) and probability (rate)
                        d[i,'pops'])) # when we finish we want to display the counts as a rate so need the denominator

#               # NOT RUN step 2 calculate the two tailed lower and upper 95% expected counts of ross river virus given the specified probability and sample size
#               ci=rbind(ci,cbind(d[i,'buffer'],
#                       qbinom(p=0.025,size=d[i,'pop'],prob=d[i,'cases']/d[i,'pop']), # qbinom is the quantile function for the binomial distribution
#                       qbinom(p=0.975,size=d[i,'pop'],prob=d[i,'cases']/d[i,'pop']),
#                       d[i,'pop'])) # for the CIs as a rate
#
}


# reshape?
head(out)
out=as.data.frame(out)
names(out)=c('index','buffer','cases','pops')
# get rid of NAs
out$cases=ifelse(is.na(out$cases),0,out$cases)

out_table=matrix(nrow=0,ncol=5)

#i=1
for(i in 1:1000){
#out[out$index==i,]
fit=glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=out[out$index==i,])

#summary(fit)

out_table=rbind(out_table,
  cbind(out[out$index==i,],predict(fit,type='response'))
  )
#out_table
}

out_table[1:16,]

output=matrix(nrow=0,ncol=4)

for(i in seq(0.5 ,7.5,0.5)){
output=rbind(output,
  cbind(i,
  quantile(out_table[out_table$buffer==i,5],0.5),
  quantile(out_table[out_table$buffer==i,5],0.05),
  quantile(out_table[out_table$buffer==i,5],0.95)
  )
)
}

output

plot(output[,2],type='b',ylim=c(0,50))
lines(output[,3],col='red')
lines(output[,4],col='red')
title(title_label)

output=as.data.frame(output, row.names = F)
names(output)=c('buffer','50pct','5pct','95pct')
output
# NOT RUN
#write.csv(output,'eastern.csv',row.names=F)
#write.csv(output,'urban.csv',row.names=F)
}
# manual loop ends here, repeat with other group
#############################################################
savePlot('reports/simulated.png')
dev.off()

# decision made to go with Poisson


#+end_src

** main analysis = do_plots_RossRiverRates.R
#+name:do_plots_RossRiverRates
#+begin_src R :session *R* :tangle code/do_plots_RossRiverRates.R :exports none :eval no
# ~/projects/Vally2012-RossRiverRates/
# EDA is in EDA_RossRiverRates.r
# this makes final plots

#################################################################################
d_eastern


fit <- glm(cases~ buffer + offset(log(pops)),family='poisson', data=d_eastern )
summa <- summary(fit)
summa
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)
## (Intercept)  -4.8244     0.1445 -33.384  < 2e-16 ***
## buffer       -0.2465     0.0792  -3.112  0.00186 **
## ---

exp(-0.2465)-1
#= -0.2184686
exp(-0.2465-1.96*0.0792)-1
#=  -0.3308399
exp(-0.2465+1.96*0.0792)-1
# =  -0.08722695


write.table('###### EASTERN','reports/output.txt',row.names=F,col.names=F,quote=F)
sink('reports/output.txt',append=T)
print(summary(fit))
sink()

png('reports/Eastern.png')
par(mar=c(4,4,1.75,1))
plot(d_eastern$buffer,(d_eastern$cases/d_eastern$pops)*1000,type='b',ylim=ylims,ylab='Incidence Rate per 1000',xlab='Buffer (Kilometres)')

lines(d_eastern$buffer,(predict(fit,type='response')/d_eastern$pops)*1000,lwd=2)

pred1  <-  predict(fit,type='link',se.fit=T)

#CIs = exp(pred1$fit-1.96*pred1$se.fit)

with(d_eastern,
  with(pred1,
    matlines(buffer,
      cbind(
        exp(fit-1.96*se.fit)/pops,
        exp(fit+1.96*se.fit)/pops
        )*1000,
      lty=2,
      col=1))
      )

legend('topright',c('Data','Model fit','95% CI'),lty=c(1,1,2),pch=c(1,NA,NA),lwd=c(1,2,1))
dev.off()


#################################################################################
d_urban

fit <- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban )
summa <- summary(fit)
summa$coeff
##               Estimate Std. Error     z value     Pr(>|z|)
## (Intercept) -5.5236291 0.33353693 -16.5607719 1.338655e-61
## buffer       0.0385268 0.06351951   0.6065349 5.441596e-01

# decided to exclude zero pop from urban
d_urban2 <- d_urban[-1,]
d_urban2

fit <- glm(cases~ buffer + offset(log(pops)),family='poisson', data=d_urban2 )

write.table('###### URBAN EXCLUDING ZERO POPULATION BUFFER 0.5','reports/output.txt',row.names=F,col.names=F,quote=F,append=T)
sink('reports/output.txt',append=T)
print(summary(fit))
sink()

png('reports/Urban.png')
par(mar=c(4,4,1.75,1))
plot(c(0.5,d_urban2$buffer),c(NA,(d_urban2$cases/d_urban2$pops)*1000),type='b',ylim=ylims, xlim = c(0,7.5),ylab='Incidence Rate per 1000',xlab='Buffer (Kilometres)')

lines(d_urban2$buffer,(predict(fit,type='response')/d_urban2$pops)*1000,lwd=2)

pred1  <-  predict(fit,type='link',se.fit=T)

with(d_urban2,
     with(pred1,
          matlines(buffer,
                   cbind(
                     exp(fit-1.96*se.fit)/pops,
                     exp(fit+1.96*se.fit)/pops
                   )*1000,
                   lty=2,
                   col=1))
)

legend('topright',c('Data','Model fit','95% CI'),lty=c(1,1,2),pch=c(1,NA,NA),lwd=c(1,2,1))
dev.off()


#+end_src

** model checking = do_model_checking.R
#+name:do_model_checking
#+begin_src R :session *R* :tangle code/do_model_checking.R :exports none :eval no
  # aims
  ## test the different way to handle the missing population row, also
  ## different parametrisations for the effect modification by urban
  ## we show that the coeffs and se are equivalent 
  
  # model 0 effect in eastern
  #d_eastern
  fit <- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_eastern )
  summa <- summary(fit)
  summa
  ## Coefficients:
  ##             Estimate Std. Error z value Pr(>|z|)
  ## (Intercept) -4.82425    0.14451 -33.382  < 2e-16 ***
  ## buffer      -0.24702    0.07921  -3.119  0.00182 **
  
  # model 1 effect in urban with dropped zero pop zone
  d_urban2 <- d_urban[-1,]
  fit1 <- glm(cases~ buffer + offset(log(pops)),family='poisson', data=d_urban2 )
  summary(fit1)
  # now  without dropping the empty pop
  fit1.1 <- glm(cases~ buffer + offset(log(1+pops)),family='poisson', data=d_urban )
  summa <- summary(fit1.1)
  summa
  ## Coefficients:
  ##             Estimate Std. Error z value Pr(>|z|)
  ## (Intercept) -5.52363    0.33354 -16.561   <2e-16 ***
  ## buffer       0.03853    0.06352   0.607    0.544
  
  
  # model 2 is a multiplicative term
  fit2 <- glm(cases ~ buffer * urban + offset(log(1+pops)), family = 'poisson', data = dat2)
  summa <- summary(fit2)
  summa
  ## Coefficients:
  ##              Estimate Std. Error z value Pr(>|z|)
  ## (Intercept)  -4.82425    0.14451 -33.382  < 2e-16 ***
  ## buffer       -0.24702    0.07921  -3.119  0.00182 **
  ## urban        -0.69938    0.36350  -1.924  0.05435 .
  ## buffer:urban  0.28555    0.10153   2.812  0.00492 **
  
  # the coeff on buffer is for urban = 0 is main effect
  # the coeff on buffer:urban is for urban = 1 is the marginal effect
  b1 <- summa$coeff[2,1]
  b3 <- summa$coeff[4,1]
  b1 + b3
  # 0.0385268
  # but what about that p-value?  and the se?
  #str(fit2)
  fit2_vcov <- vcov(fit2)
  #fit2_vcov
  # now calculate the conditional standard error for the marginal effect of buffer for the value of the modifying variable (Z, urban =1)
  varb1<-fit2_vcov[2,2]
  varb3<-fit2_vcov[4,4]
  covarb1b3<-fit2_vcov[2,4]
  Z<-1
  conditional_se <- sqrt(varb1+varb3*(Z^2)+2*Z*covarb1b3)
  conditional_se
  
  
  
  # model 3 is the re-parametrisation
  dat2$buffer_urban <- dat2$buffer * dat2$urban
  dat2$buffer_eastern <- dat2$buffer * (1-dat2$urban)
  
  fit3 <- glm(cases ~ buffer_urban + buffer_eastern + urban + offset(log(1+pops)), family = 'poisson', data = dat2)
  summa <- summary(fit3)
  summa
  
  ## Coefficients:
  ##                Estimate Std. Error z value Pr(>|z|)
  ## (Intercept)    -4.82425    0.14451 -33.382  < 2e-16 ***
  ## buffer_urban    0.03853    0.06352   0.607  0.54416
  ## buffer_eastern -0.24702    0.07921  -3.119  0.00182 **
  ## urban          -0.69938    0.36350  -1.924  0.05435 .
  
  
#+end_src

* report.Rmd
#+name:main
#+begin_src R :session *R* :tangle main.Rmd :exports none :eval no :padline no
---
title: "Ross River virus paper: model checking"
author: "ivanhanigan"
date: "3/11/2015"
output: html_document
---

# Introduction 

This is a report of the model checking performed for the paper:  Vally, H., Peel, M., Dowse, G. K., Cameron, S., Codde, J. P., Hanigan, I., \& Lindsay, M. D. a. (2012). Geographic information systems used to describe the link between the risk of Ross River virus infection and proximity to the Leschenault Estuary, WA. Australian and New Zealand Journal of Public Health, 36(3), 229–235. doi:10.1111/j.1753-6405.2012.00869.x

```{r, echo = FALSE, results = 'hide'}
source("main.R")
# this just re-ran all the analysis
# the alternate models are in the do_model_checking.R file
# for the report we need xtable
library(xtable)
```  

# The issues

- In original modelling I chose to drop a buffer in the urban zone that had zero population
- This could have been dealt with by adding one to the `offset(log(pop))`
- The difference in the two models is shown:

```{r, results = 'asis'}
# the first time I dropped that row
fit1$call
print(xtable(fit1), type = 'html')

# this time I have kept it
fit1.1$call
print(xtable(fit1.1), type = 'html')
```
  
#+end_src


